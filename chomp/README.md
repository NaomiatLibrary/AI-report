# chomp solver
チョンプを解くためのコードです。

## 3x5のチョンプ
必勝法があることを示すコードを書きました。
### 実行方法
```
g++ -Wall -std=c++14 35chomp.cpp
./a.out > ans_35chomp.txt
```
### 結果
`ans_35chomp.txt`に書いてある通り、最初の手が
```
+-+-+-+-+-+
| | | |/|/|
+-+-+-+-+-+
| | | | | |
+-+-+-+-+-+
|*| | | | |
+-+-+-+-+-+
```
であるような必勝法が存在することがわかる。(その後の手は相手の出し方によって異なる)
また、最も勝つまでの手数が長い試合(9手)のうち一つを記した。

## nxnのチョンプ
### 実行方法
```
g++ -Wall -std=c++14 <filename>
./a.out
```
`nnchomp_mmvsmm`: 単純なmin-max探索
`nnchomp_abmmvsabmm`: alpha-betaカット付きmin-max法による探索
`nnchonp_mcvsabmm`: 原始モンテカルロ木探索とalpha-betaカット付きmin-max法による対戦(100回)
`nnchomp_mcvsUCT`: 原始モンテカルロ木探索とUCT法探索による対戦

### alpha-betaカットによる高速化
5x5のチョンプを、深さ10のmin-max探索によって解いた。
（ここでの評価関数は、とりあえず勝ったときに100,負けたときに-100という評価のみとした。）
`nnchomp_mmvsmm.cpp`(単純なmin-max探索同士による対決)と、`nnchomp_abmmvsabmm.cpp`(alpha-betaカットを行なったmin-max探索同士による対決)を実行し、各手法において出力された手とどのくらい探索に時間がかかったかを出力したものが以下になる。

単純なmin-max探索同士の対決の結果: `ans_mmvsmm.txt`参照

alpha-betaカットを行なった対決の結果: `ans_abmmvsabmm.txt`参照

結果は全く同じであるが、全体的にalpha-betaカットの方が探索時間が短くなっていることがわかる。
特に初手において、単純なmin-max探索では461秒もかかっているが、alpha-betaカットを行うことによって0.1秒程度に抑えられていることがわかる。
これを利用して、alpha-betaカットではより深い探索を行いより強いAIを構築できると考えられる。

### 原始モンテカルロ木探索とalpha-betaカットの比較
原始モンテカルロ木探索と先ほどのalpha-betaカットのAIを、先手後手入れ替えそれぞれ100回対戦させて、モンテカルロ木探索の勝率を調べた。
min-max法は深さ5まで、モンテカルロ法のプレイアウトは1000回行う。
ソースコードは`nnchomp_mcvsabmm.cpp`である。

結果
```
N=3の時
後手　原始モンテカルロの勝率 15 percent
先手　原始モンテカルロの勝率 69 percent
N=5の時
後手　原始モンテカルロの勝率 19 percent
先手　原始モンテカルロの勝率 13 percent
N=8の時
後手　原始モンテカルロの勝率 15 percent
先手　原始モンテカルロの勝率 8 percent
N=10の時
後手　原始モンテカルロの勝率 13 percent
先手　原始モンテカルロの勝率 10 percent
```
探索木の深さとプレイアウト回数によっても異なるだろうが、今回はalpha-betaカットの方が圧倒的に強くなった。これは、チョンプにおいて相手は極端に不利な手（毒のある部分を噛む）をすることができるため、モンテカルロでランダムに手を選んだ時はその手を相手が選択する場合を考慮してしまうが、実際にはそのようなことが起こらないことが原因だと考えられる。また、原始モンテカルロが先手の時、盤面を広げると、原始モンテカルロの勝率も下がった。可能性のある局面が多いときには、ランダムに選ばれた局面の数が相対的に少なくなってしまい、手の良さを予測するのに十分でなくなるからであると考えられる。

## 原始モンテカルロ木探索とUCT探索の比較
原始モンテカルロ木探索とUCT探索を100回数対戦させ、勝率を調べたところ、以下のようになった
(c=0.5,プレイアウト回数100)
```
N=3の時
先手　UCTの勝率 70 percent
後手 UCTの勝率 40 percent
N=5の時
先手　UCTの勝率 50 percent
後手 UCTの勝率 55 percent
N=10の時
先手　UCTの勝率 56 percent
後手 UCTの勝率 43 percent
```
あまり変わらなかった。

## より良い評価関数の吟味
以上の試行より、alpha-betaカット付きmin-max法の評価関数を改善することがチョンプにおいては最も効果的だと感じたので、評価関数を色々用意して試行してみた。
N=10,探索の深さは4で実行した。

評価関数を
残りの噛めるところが奇数だったらMY_TURNの時-10,YOUR_TURNの時+10
にしたもの
```
先手　原始評価関数の勝率 0 percent
後手 原始評価関数の勝率 100 percent
```
原始評価関数によるものより強いとは思えない結果となった。

評価関数を
残りの噛めるところが奇数だったらMY_TURNの時-(すでに噛んであるブロックの総数),YOUR_TURNの時+(すでに噛んであるブロックの総数)を返す。偶数の時、逆
にしたものだと、
```
先手　原始評価関数の勝率 0 percent
後手 原始評価関数の勝率 0 percent
```
かなり強くなったと思われる。これと原始モンテカルロを対戦させたところ、

```
N=3の時
後手　原始モンテカルロの勝率 0 percent
先手　原始モンテカルロの勝率 75 percent
N=5の時
後手　原始モンテカルロの勝率 0 percent
先手　原始モンテカルロの勝率 4 percent
N=8の時
後手　原始モンテカルロの勝率 0 percent
先手　原始モンテカルロの勝率 0 percent
N=10の時
後手　原始モンテカルロの勝率 0 percent
先手　原始モンテカルロの勝率 0 percent
```
明らかに評価関数を変えたことで強くなった。ほぼモンテカルロ法を完封している。

